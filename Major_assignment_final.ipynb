{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageFile\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, ViTImageProcessor, ViTModel\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemeDataset(Dataset):\n",
    "    def __init__(self, labels_path, image_dir, text_max_length=128):\n",
    "        self.labels_df = pd.read_csv(labels_path)\n",
    "        self.image_dir = image_dir\n",
    "        \n",
    "        # Data validation\n",
    "        self.labels_df['text_corrected'] = self.labels_df['text_corrected'].astype(str)\n",
    "        \n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        self.text_max_length = text_max_length\n",
    "        \n",
    "        self.image_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "        \n",
    "        self.label_maps = {\n",
    "            'sentiment': ['very_negative', 'negative', 'neutral', 'positive', 'very_positive'],\n",
    "            'humor': ['not_funny', 'funny', 'very_funny', 'hilarious'],\n",
    "            'sarcasm': ['not_sarcastic', 'general', 'twisted_meaning', 'very_twisted'],\n",
    "            'offensive': ['not_offensive', 'slight', 'very_offensive', 'hateful_offensive'],\n",
    "            'motivational': ['not_motivational', 'motivational']\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.labels_df.iloc[idx]\n",
    "        \n",
    "        # Text processing\n",
    "        text = str(row['text_corrected'])\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.text_max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Image processing\n",
    "        img_path = os.path.join(self.image_dir, row['image_name'])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        pixel_values = self.image_processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "        \n",
    "        # Label encoding\n",
    "        labels = {\n",
    "            'sentiment': torch.tensor(self.label_maps['sentiment'].index(row['overall_sentiment']), dtype=torch.long),\n",
    "            'humor': torch.tensor(self.label_maps['humor'].index(row['humour']), dtype=torch.long),\n",
    "            'sarcasm': torch.tensor(self.label_maps['sarcasm'].index(row['sarcasm']), dtype=torch.long),\n",
    "            'offensive': torch.tensor(self.label_maps['offensive'].index(row['offensive']), dtype=torch.long),\n",
    "            'motivational': torch.tensor(1 if row['motivational'] == 'motivational' else 0, dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'pixel_values': pixel_values.squeeze(),\n",
    "            'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Text encoder\n",
    "        self.text_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        \n",
    "        # Image encoder\n",
    "        self.image_model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "        \n",
    "        # Multimodal fusion\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(768*2, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        # Classification heads\n",
    "        self.classifier = nn.ModuleDict({\n",
    "            'sentiment': nn.Linear(512, 5),\n",
    "            'humor': nn.Linear(512, 4),\n",
    "            'sarcasm': nn.Linear(512, 4),\n",
    "            'offensive': nn.Linear(512, 4),\n",
    "            'motivational': nn.Linear(512, 2)\n",
    "        })\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, pixel_values):\n",
    "        text_out = self.text_model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:,0,:]\n",
    "        image_out = self.image_model(pixel_values=pixel_values).last_hidden_state[:,0,:]\n",
    "        \n",
    "        fused = torch.cat([text_out, image_out], dim=1)\n",
    "        fused = self.fusion(fused)\n",
    "        \n",
    "        return {task: self.classifier[task](fused) for task in self.classifier}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, criterion, epochs=2):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "            labels = {k: v.to(device) for k, v in batch['labels'].items()}\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            loss = sum(criterion[task](outputs[task], labels[task]) for task in outputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} Avg Loss: {total_loss/len(dataloader):.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = {task: [] for task in criterion}, {task: [] for task in criterion}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "            labels = {k: v.cpu().numpy() for k, v in batch['labels'].items()}\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            preds = {task: torch.argmax(outputs[task], dim=1).cpu().numpy() for task in outputs}\n",
    "\n",
    "            for task in preds:\n",
    "                all_preds[task].extend(preds[task])\n",
    "                all_labels[task].extend(labels[task])\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {}\n",
    "    for task in criterion:\n",
    "        metrics[task] = {\n",
    "            \"Accuracy\": accuracy_score(all_labels[task], all_preds[task]),\n",
    "            \"F1 Score\": f1_score(all_labels[task], all_preds[task], average=\"macro\"),\n",
    "        }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = MemeDataset(labels_path='D:/Multimodal Sentiment Analysis/Major Assignment 3/Multimodal_dataset_assignment3/Multimodal_dataset_assignment3/labels.csv', image_dir='D:/Multimodal Sentiment Analysis/Major Assignment 3/Multimodal_dataset_assignment3/Multimodal_dataset_assignment3/images')\n",
    "# dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "labels_path = 'C:/Users/rouna/OneDrive/Desktop/Shashi Project/Multimodal_dataset_assignment3/Multimodal_dataset_assignment3/labels.csv'\n",
    "image_dir = 'C:/Users/rouna/OneDrive/Desktop/Shashi Project/Multimodal_dataset_assignment3/Multimodal_dataset_assignment3/images'\n",
    "\n",
    "full_df = pd.read_csv(labels_path)\n",
    "\n",
    "# Split data into train and validation sets\n",
    "train_df, val_df = train_test_split(full_df, test_size=0.2, random_state=42, stratify=full_df['overall_sentiment'])\n",
    "\n",
    "# Save split datasets to new CSV files\n",
    "train_df.to_csv(\"train_labels.csv\", index=False)\n",
    "val_df.to_csv(\"val_labels.csv\", index=False)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MemeDataset(labels_path='train_labels.csv', image_dir=image_dir)\n",
    "val_dataset = MemeDataset(labels_path='val_labels.csv', image_dir=image_dir)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultimodalModel().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = {\n",
    "    task: nn.CrossEntropyLoss()\n",
    "    for task in ['sentiment', 'humor', 'sarcasm', 'offensive', 'motivational']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  46%|████▌     | 160/350 [02:06<02:15,  1.40it/s, loss=5.6942]c:\\Users\\rouna\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\PIL\\Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 1/2: 100%|██████████| 350/350 [04:21<00:00,  1.34it/s, loss=5.7952]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Avg Loss: 5.6566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|██████████| 350/350 [04:27<00:00,  1.31it/s, loss=5.7108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Avg Loss: 5.5605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# trained_model = train_model(model, dataloader, optimizer, criterion)\n",
    "trained_model = train_model(model, train_dataloader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "torch.save(trained_model.state_dict(), 'memotion_model.pth')\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultimodalModel(\n",
       "  (text_model): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (image_model): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTSdpaAttention(\n",
       "            (attention): ViTSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (fusion): Sequential(\n",
       "    (0): Linear(in_features=1536, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (classifier): ModuleDict(\n",
       "    (sentiment): Linear(in_features=512, out_features=5, bias=True)\n",
       "    (humor): Linear(in_features=512, out_features=4, bias=True)\n",
       "    (sarcasm): Linear(in_features=512, out_features=4, bias=True)\n",
       "    (offensive): Linear(in_features=512, out_features=4, bias=True)\n",
       "    (motivational): Linear(in_features=512, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model for evaluation\n",
    "model.load_state_dict(torch.load(\"memotion_model.pth\", map_location=device))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 88/88 [00:39<00:00,  2.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# metrics = evaluate_model(model, test_dataloader)\n",
    "metrics = evaluate_model(model, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Task: sentiment\n",
      "  Accuracy: 0.4475\n",
      "  Macro F1 Score: 0.1237\n",
      "\n",
      "Task: humor\n",
      "  Accuracy: 0.3495\n",
      "  Macro F1 Score: 0.1400\n",
      "\n",
      "Task: sarcasm\n",
      "  Accuracy: 0.5118\n",
      "  Macro F1 Score: 0.1693\n",
      "\n",
      "Task: offensive\n",
      "  Accuracy: 0.3896\n",
      "  Macro F1 Score: 0.1891\n",
      "\n",
      "Task: motivational\n",
      "  Accuracy: 0.6576\n",
      "  Macro F1 Score: 0.3967\n"
     ]
    }
   ],
   "source": [
    "for task, scores in metrics.items():\n",
    "    print(f\"\\nTask: {task}\")\n",
    "    print(f\"  Accuracy: {scores['Accuracy']:.4f}\")\n",
    "    print(f\"  Macro F1 Score: {scores['F1 Score']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
